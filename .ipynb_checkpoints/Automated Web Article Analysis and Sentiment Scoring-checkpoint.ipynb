{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import csv\n",
    "import re\n",
    "from docx import Document\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Rakshitha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Rakshitha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved document for URL ID: Article1\n",
      "Saved document for URL ID: Article2\n",
      "Saved document for URL ID: Article3\n",
      "Saved document for URL ID: Article4\n",
      "Saved document for URL ID: Article5\n",
      "Saved document for URL ID: Article6\n",
      "Saved document for URL ID: Article7\n",
      "Saved document for URL ID: Article8\n",
      "Saved document for URL ID: Article9\n",
      "Saved document for URL ID: Article10\n",
      "Saved document for URL ID: Article11\n",
      "Saved document for URL ID: Article12\n",
      "Saved document for URL ID: Article13\n",
      "Saved document for URL ID: Article14\n",
      "Saved document for URL ID: Article15\n",
      "All documents saved successfully.\n",
      "Scores saved to C:\\Users\\Rakshitha\\Automated Web Article Analysis\\Output.csv\n",
      "Scores saved to C:\\Users\\Rakshitha\\Automated Web Article Analysis\\Output.csv\n",
      "Scores saved to C:\\Users\\Rakshitha\\Automated Web Article Analysis\\Output.csv\n",
      "Scores saved to C:\\Users\\Rakshitha\\Automated Web Article Analysis\\Output.csv\n",
      "Scores saved to C:\\Users\\Rakshitha\\Automated Web Article Analysis\\Output.csv\n",
      "Scores saved to C:\\Users\\Rakshitha\\Automated Web Article Analysis\\Output.csv\n",
      "Scores saved to C:\\Users\\Rakshitha\\Automated Web Article Analysis\\Output.csv\n",
      "Scores saved to C:\\Users\\Rakshitha\\Automated Web Article Analysis\\Output.csv\n",
      "Scores saved to C:\\Users\\Rakshitha\\Automated Web Article Analysis\\Output.csv\n",
      "Scores saved to C:\\Users\\Rakshitha\\Automated Web Article Analysis\\Output.csv\n",
      "Scores saved to C:\\Users\\Rakshitha\\Automated Web Article Analysis\\Output.csv\n",
      "Scores saved to C:\\Users\\Rakshitha\\Automated Web Article Analysis\\Output.csv\n",
      "Scores saved to C:\\Users\\Rakshitha\\Automated Web Article Analysis\\Output.csv\n",
      "Scores saved to C:\\Users\\Rakshitha\\Automated Web Article Analysis\\Output.csv\n",
      "Scores saved to C:\\Users\\Rakshitha\\Automated Web Article Analysis\\Output.csv\n"
     ]
    }
   ],
   "source": [
    "# Download required NLTK data files\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Function to get the title and article content from a given link\n",
    "def fetch_article(link):\n",
    "    page = requests.get(link)\n",
    "    soup = bs(page.text, 'html.parser')\n",
    "\n",
    "    # Extract the title\n",
    "    title = soup.find('h1', class_='article-title font-primary text-secondary-900 text-4xl lg:text-15xl font-bold leading-2xl lg:leading-15xl mb-3')\n",
    "    if not title:\n",
    "        title = soup.find('h1', class_='featured-template-title entry-title text-center text-2xl max-w-300 md:max-w-720 lg:text-15xl font-bold font-primary leading-2xl lg:leading-15xl mb-4 text-secondary-900 md:mx-4 lg:mx-0')\n",
    "    title_text = title.get_text() if title else \"No Title Found\"\n",
    "\n",
    "    # Extract the article content\n",
    "    article = soup.find('div', class_=\"content-wrapper\")\n",
    "    article_content = article.get_text() if article else \"No Content Found\"\n",
    "    \n",
    "    return title_text, article_content\n",
    "\n",
    "# Function to find hyperlinks in a cell\n",
    "def find_hyperlinks(cell_value):\n",
    "    return re.findall(r'(https?://[^\\s]+)', cell_value)\n",
    "\n",
    "# Function to find URL IDs in a cell\n",
    "def find_url_ids(cell_value):\n",
    "    return re.findall(r'Article[0-9]+', cell_value)\n",
    "\n",
    "# Path to the Input CSV file\n",
    "csv_file_path = \"C:\\\\Users\\\\Rakshitha\\\\Automated Web Article Analysis\\\\Input.csv\"\n",
    "\n",
    "# Read the CSV file\n",
    "with open(csv_file_path, mode='r', encoding='utf-8') as file:\n",
    "    reader = csv.reader(file)\n",
    "    data = list(reader)\n",
    "\n",
    "# Function to find hyperlinks and URL IDs in a row\n",
    "def find_links_and_ids(row):\n",
    "    links, ids = [], []\n",
    "    for cell in row:\n",
    "        cell_links = find_hyperlinks(cell)\n",
    "        cell_ids = find_url_ids(cell)\n",
    "        if cell_links:\n",
    "            links.extend(cell_links)\n",
    "        if cell_ids:\n",
    "            ids.extend(cell_ids)\n",
    "    return links, ids\n",
    "\n",
    "# Extract hyperlinks and URL IDs\n",
    "all_hyperlinks, all_url_ids = [], []\n",
    "for row in data:\n",
    "    links, ids = find_links_and_ids(row)\n",
    "    all_hyperlinks.extend(links)\n",
    "    all_url_ids.extend(ids)\n",
    "\n",
    "# Ensure the same length of hyperlinks and URL IDs\n",
    "if len(all_hyperlinks) != len(all_url_ids):\n",
    "    print(\"Mismatch between the number of hyperlinks and URL IDs\")\n",
    "else:\n",
    "    docx_paths = []  # List to store paths of created docx files\n",
    "    for link, url_id in zip(all_hyperlinks, all_url_ids):\n",
    "        try:\n",
    "            # Fetch the article text\n",
    "            title, article_content = fetch_article(link)\n",
    "        \n",
    "            # Create a new Word document\n",
    "            doc = Document()\n",
    "            doc.add_heading(title, 0)\n",
    "            doc.add_paragraph(article_content)\n",
    "        \n",
    "            # Save the document with the URL ID as the filename\n",
    "            docx_path = f\"{url_id}.docx\"\n",
    "            doc.save(docx_path)\n",
    "            docx_paths.append(docx_path)\n",
    "\n",
    "            print(f\"Saved document for URL ID: {url_id}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {link} with URL ID {url_id}: {e}\")\n",
    "\n",
    "print(\"All documents saved successfully.\")\n",
    "\n",
    "# Function to extract text from a docx file\n",
    "def extract_text_from_docx(docx_path):\n",
    "    doc = Document(docx_path)\n",
    "    return ' '.join(paragraph.text for paragraph in doc.paragraphs)\n",
    "\n",
    "# Function to remove stopwords from text\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = text.split()\n",
    "    return ' '.join(word for word in words if word.lower() not in stop_words)\n",
    "\n",
    "# Process each document\n",
    "for docx_path in docx_paths:\n",
    "    extracted_text = extract_text_from_docx(docx_path)\n",
    "    cleaned_text = remove_stopwords(extracted_text)\n",
    "\n",
    "# Function to load positive and negative sentiment dictionaries\n",
    "def load_sentiment_dictionaries():\n",
    "    positive_words, negative_words = set(), set()\n",
    "    \n",
    "    with open(\"C:\\\\Users\\\\Rakshitha\\\\Automated Web Article Analysis\\\\MasterDictionary\\\\positive-words.txt\", 'r', encoding='latin-1') as f_pos:\n",
    "        for line in f_pos:\n",
    "            positive_words.add(line.strip().lower())\n",
    "    \n",
    "    with open(\"C:\\\\Users\\\\Rakshitha\\\\Automated Web Article Analysis\\\\MasterDictionary\\\\negative-words.txt\", 'r', encoding='latin-1') as f_neg:\n",
    "        for line in f_neg:\n",
    "            negative_words.add(line.strip().lower())\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return positive_words - stop_words, negative_words - stop_words\n",
    "\n",
    "# Function to calculate sentiment scores\n",
    "def calculate_scores(cleaned_text):\n",
    "    positive_words, negative_words = load_sentiment_dictionaries()\n",
    "    tokens = nltk.word_tokenize(cleaned_text.lower())\n",
    "    \n",
    "    positive_score = sum(1 for word in tokens if word in positive_words)\n",
    "    negative_score = sum(1 for word in tokens if word in negative_words)\n",
    "    polarity_score = (positive_score - negative_score) / (positive_score + negative_score + 0.000001)\n",
    "    subjectivity_score = (positive_score + negative_score) / (len(tokens) + 0.000001)\n",
    "    \n",
    "    return positive_score, negative_score, polarity_score, subjectivity_score\n",
    "\n",
    "# Function to count syllables in a word\n",
    "def count_syllables(word):\n",
    "    word = word.lower()\n",
    "    syllable_count = len(re.findall(r'[aeiouy]+', word))\n",
    "    if word.endswith('e'):\n",
    "        syllable_count -= 1\n",
    "    if word.endswith (('es', 'ed')) and len(word) > 2:\n",
    "        syllable_count -= 1\n",
    "    return max(1, syllable_count)\n",
    "\n",
    "# Function to count complex words\n",
    "def count_complex_words(word_list):\n",
    "    return sum(1 for word in word_list if count_syllables(word) > 2)\n",
    "\n",
    "# Function to calculate readability metrics\n",
    "def calculate_readability_metrics(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    personal_pronouns = re.compile(r'\\b(I|we|my|ours|us)\\b', re.IGNORECASE)\n",
    "    \n",
    "    sentences = sent_tokenize(text)\n",
    "    words = word_tokenize(text)\n",
    "    cleaned_words = [word for word in words if word.isalpha() and word.lower() not in stop_words]\n",
    "    \n",
    "    num_sentences = len(sentences)\n",
    "    num_words = len(cleaned_words)\n",
    "    num_complex_words = count_complex_words(cleaned_words)\n",
    "    syllable_counts = [count_syllables(word) for word in cleaned_words]\n",
    "    personal_pronoun_count = len(personal_pronouns.findall(text))\n",
    "    \n",
    "    avg_sentence_length = num_words / num_sentences\n",
    "    avg_words_per_sentence = num_words / num_sentences\n",
    "    percentage_complex_words = (num_complex_words / num_words) * 100\n",
    "    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)  # Corrected line\n",
    "    avg_word_length = sum(len(word) for word in cleaned_words) / num_words\n",
    "    word_count = num_words\n",
    "    \n",
    "    return {\n",
    "        \"avg_sentence_length\": avg_sentence_length,\n",
    "        \"avg_words_per_sentence\": avg_words_per_sentence,\n",
    "        \"percentage_complex_words\": percentage_complex_words,\n",
    "        \"fog_index\": fog_index,\n",
    "        \"avg_word_length\": avg_word_length,\n",
    "        \"word_count\": word_count,\n",
    "        \"num_complex_words\": num_complex_words,\n",
    "        \"syllable_counts\": syllable_counts,\n",
    "        \"personal_pronoun_count\": personal_pronoun_count\n",
    "    }\n",
    "\n",
    "# Function to save scores to a CSV file\n",
    "def save_scores_to_csv(file_path, scores):\n",
    "    columns = ['URL ID', 'URL', 'Positive Score', 'Negative Score', 'Polarity Score', 'Subjectivity Score',\n",
    "               'Average Sentence Length', 'Average Words Per Sentence', 'Percentage of Complex Words',\n",
    "               'Fog Index', 'Average Word Length', 'Word Count', 'Complex Word Count', 'Total Syllables',\n",
    "               'Personal Pronoun Count']\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "    except (FileNotFoundError, pd.errors.EmptyDataError):\n",
    "        df = pd.DataFrame(columns=columns)\n",
    "    \n",
    "    new_df = pd.DataFrame([scores], columns=columns)\n",
    "    df = pd.concat([df, new_df], ignore_index=True)\n",
    "    \n",
    "    df.to_csv(file_path, index=False)\n",
    "    print(f\"Scores saved to {file_path}\")\n",
    "\n",
    "# Process each document and save scores\n",
    "for docx_path in docx_paths:\n",
    "    extracted_text = extract_text_from_docx(docx_path)\n",
    "    cleaned_text = remove_stopwords(extracted_text)\n",
    "    \n",
    "    pos_score, neg_score, polarity_score, subjectivity_score = calculate_scores(cleaned_text)\n",
    "    metrics = calculate_readability_metrics(cleaned_text)\n",
    "    \n",
    "    url_id = re.findall(r'Article[0-9]+', docx_path)[0]\n",
    "    link = all_hyperlinks[all_url_ids.index(url_id)]\n",
    "    readability_scores = list(metrics.values())\n",
    "    scores = [url_id, link, pos_score, neg_score, polarity_score, subjectivity_score] + readability_scores\n",
    "    \n",
    "    csv_output_path = \"C:\\\\Users\\\\Rakshitha\\\\Automated Web Article Analysis\\\\Output.csv\"\n",
    "    save_scores_to_csv(csv_output_path, scores)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
